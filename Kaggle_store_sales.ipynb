{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "292ee6f3",
   "metadata": {},
   "source": [
    "# Passos\n",
    "\n",
    "# Objetivo: Prever próximos 15 dias\n",
    "\n",
    "# 1°Coleta e Preparação dos Dados\n",
    "\n",
    "## Analisar dados importantes\n",
    "\n",
    "## Procurar por valores ausentes e outliers\n",
    "\n",
    "## Realizar a conversão e manipulação de dados\n",
    "\n",
    "# 2°Análise Exploratória de Dados (EDA)\n",
    "\n",
    "## Visualizar os dados.\n",
    "\n",
    "## Verificar a estacionaridade e sazonalidade.\n",
    "\n",
    "## Analisar os padrões, tendências e ciclos.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f255f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e4f2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.power import tt_ind_solve_power\n",
    "from scipy.stats import ks_2samp\n",
    "from scipy.stats import spearmanr\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.diagnostic import linear_harvey_collier\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.metrics import r2_score\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "#statistics libraries\n",
    "import scipy\n",
    "from statsmodels.tools.eval_measures import rmse\n",
    "from statsmodels.graphics.tsaplots import month_plot, seasonal_plot, plot_acf, plot_pacf, quarter_plot\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox as ljung\n",
    "#from nimbusml.timeseries import SsaForecaster\n",
    "from statsmodels.tsa.statespace.tools import diff as diff\n",
    "from scipy import signal\n",
    "from scipy.stats import shapiro\n",
    "from scipy.stats import boxcox\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import jarque_bera as jb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import r2_score, median_absolute_error, mean_absolute_error\n",
    "from sklearn.metrics import median_absolute_error, mean_squared_error, mean_squared_log_error, mean_absolute_error\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673b6c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataframes = [\"holidays_events.csv\", \"oil.csv\", \"sample_submission.csv\", \"stores.csv\", \"test.csv\", \"train.csv\", \"transactions.csv\"]\n",
    "\n",
    "for i in dataframes:\n",
    "    # Retirar a extensão do nome do arquivo para usar como nome da variável\n",
    "    var_name = i.split('.')[0]\n",
    "    # Utilizar globals() para criar uma variável com esse nome e atribuir o DataFrame a ela\n",
    "    globals()[var_name] = pd.read_csv(i)\n",
    "    print(globals()[var_name].head())\n",
    "    print()\n",
    "    print(globals()[var_name].info())\n",
    "    print()\n",
    "    print(globals()[var_name].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1175c0",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6164de0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcdd3fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f8a6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['date']=pd.to_datetime(train['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a83c4f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38cf497",
   "metadata": {},
   "outputs": [],
   "source": [
    "effect_size = 0.1  # Tamanho do efeito desejado\n",
    "alpha = 0.05       # Nível de significância\n",
    "power = 0.95       # Poder desejado\n",
    "\n",
    "sample_size = tt_ind_solve_power(effect_size=effect_size, alpha=alpha, power=power)\n",
    "\n",
    "print(f\"Tamanho da amostra necessário: {sample_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ae014c",
   "metadata": {},
   "source": [
    "# Tamanho suficiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e736571",
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays_events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9514293",
   "metadata": {},
   "source": [
    "sns.histplot(train['sales'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e339fb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train[train.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7e5976",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train['sales'].quantile([0.25, 0.5, 0.75, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6119c6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(train['sales'], bins=50, edgecolor='black')\n",
    "plt.xlabel('Sales')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Sales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "832a5b74",
   "metadata": {},
   "source": [
    "def remove_initial_zeros(group):\n",
    "    # Identificar o índice do primeiro valor não-zero\n",
    "    first_non_zero_idx = group['sales'].ne(0).idxmax()\n",
    "    # Retornar o subconjunto do DataFrame a partir desse índice\n",
    "    return group.loc[first_non_zero_idx:]\n",
    "\n",
    "# Agrupar por store_nbr e family, e aplicar a função a cada grupo\n",
    "train = train.groupby(['store_nbr', 'family']).apply(remove_initial_zeros).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aeeb50d8",
   "metadata": {},
   "source": [
    "Q1 = train['sales'].quantile(0.25)\n",
    "Q3 = train['sales'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = 0\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "# Substituindo os outliers pelas bordas do intervalo aceitável (você também pode escolher outras formas de tratar os outliers)\n",
    "train['sales'] = train['sales'].apply(lambda x: upper_bound if x > upper_bound else x)\n",
    "train['sales'] = train['sales'].apply(lambda x: lower_bound if x < lower_bound else x)\n",
    "\n",
    "# Agora o DataFrame 'train' está limpo e pronto para análise e modelagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c37ae2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(train['sales'], bins=50, edgecolor='black')\n",
    "plt.xlabel('Sales')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Sales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044110c6",
   "metadata": {},
   "source": [
    "# Analisando transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318b4f9b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "transactions.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8e6a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions['date']=pd.to_datetime(transactions['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79b76c3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "temp = pd.merge(train.groupby([\"date\", \"store_nbr\"]).sales.sum().reset_index(), transactions, how = \"left\")\n",
    "print(\"Correlação de Spearman de vendas totais e transações {:,.4f}\".format(temp.corr(\"spearman\").sales.loc[\"transactions\"]))\n",
    "px.line(transactions.sort_values([\"store_nbr\", \"date\"]), x='date', y='transactions', color='store_nbr',title = \"Transactions\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931d42ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = transactions.copy()\n",
    "a[\"year\"] = a.date.dt.year\n",
    "a[\"month\"] = a.date.dt.month\n",
    "px.box(a, x=\"year\", y=\"transactions\" , color = \"month\", title = \"Transactions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f981ce5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = transactions.set_index(\"date\").resample(\"M\").transactions.mean().reset_index()\n",
    "a[\"year\"] = a.date.dt.year\n",
    "px.line(a, x='date', y='transactions', color='year',title = \"Monthly Average Transactions\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c54980",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "px.scatter(temp, x = \"transactions\", y = \"sales\", trendline = \"ols\", trendline_color_override = \"red\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4fd391",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = transactions.copy()\n",
    "a[\"year\"] = a.date.dt.year\n",
    "a[\"dayofweek\"] = a.date.dt.dayofweek+1\n",
    "a = a.groupby([\"year\", \"dayofweek\"]).transactions.mean().reset_index()\n",
    "px.line(a, x=\"dayofweek\", y=\"transactions\" , color = \"year\", title = \"Transactions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b58dab1",
   "metadata": {},
   "source": [
    "É observável uma relação de sazonalidade tanto mensal, como semanal também."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1644c143",
   "metadata": {},
   "source": [
    "# Dando uma olhada nos dfs de feriados e óleo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6e2d9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "oil.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd82614",
   "metadata": {},
   "outputs": [],
   "source": [
    "oil['date']=pd.to_datetime(oil['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864dfe40",
   "metadata": {},
   "outputs": [],
   "source": [
    "oil.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684ec99c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.hist(oil['dcoilwtico'], bins=50, edgecolor='black')\n",
    "plt.xlabel('Oil Price')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Oil Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0643c571",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Interpolate\n",
    "oil[\"dcoilwtico\"] = np.where(oil[\"dcoilwtico\"] == 0, np.nan, oil[\"dcoilwtico\"])\n",
    "oil[\"dcoilwtico_interpolated\"] =oil.dcoilwtico.interpolate()\n",
    "# Plot\n",
    "p = oil.melt(id_vars=['date']+list(oil.keys()[5:]), var_name='Legend')\n",
    "px.line(p.sort_values([\"Legend\", \"date\"], ascending = [False, True]), x='date', y='value', color='Legend',title = \"Daily Oil Price\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f09688",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.merge(temp, oil, how = \"left\")\n",
    "print(\"Correlation with Daily Oil Prices\")\n",
    "print(temp.drop([\"store_nbr\", \"dcoilwtico\"], axis = 1).corr(\"spearman\").dcoilwtico_interpolated.loc[[\"sales\", \"transactions\"]], \"\\n\")\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize = (15,5))\n",
    "temp.plot.scatter(x = \"dcoilwtico_interpolated\", y = \"transactions\", ax=axes[0])\n",
    "temp.plot.scatter(x = \"dcoilwtico_interpolated\", y = \"sales\", ax=axes[1], color = \"r\")\n",
    "axes[0].set_title('Daily oil price & Transactions', fontsize = 15)\n",
    "axes[1].set_title('Daily Oil Price & Sales', fontsize = 15);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbddfa3",
   "metadata": {},
   "source": [
    "# Agora observando os feriados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1a2cb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "holidays_events.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93436196",
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays_events['date']=pd.to_datetime(holidays_events['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f357582",
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays_events=holidays_events[holidays_events['transferred']==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcec5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays_events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e9769d",
   "metadata": {},
   "source": [
    "# Analisando eventos que podem afetar a análise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8801ca4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['payday'] = ((train['date'].dt.day == 15) | (train['date'].dt.is_month_end)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008a5b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquake_date = pd.Timestamp('2016-04-16')\n",
    "train['earthquake_effect'] = ((train['date'] > earthquake_date) & (train['date'] < earthquake_date + pd.Timedelta(weeks=4))).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764e16fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.merge(oil, on='date', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66825be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['date']=pd.to_datetime(test['date'])\n",
    "test=test.merge(oil, on='date', how='left')\n",
    "test['payday'] = ((test['date'].dt.day == 15) | (test['date'].dt.is_month_end)).astype(int)\n",
    "test['dcoilwtico'] = test['dcoilwtico'].fillna(method='ffill')\n",
    "test['holiday'] = test['date'].isin(holidays_events['date']).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5b0ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['dcoilwtico'] = train['dcoilwtico'].fillna(method='ffill')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bbebc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['holiday'] = train['date'].isin(holidays_events['date']).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8437a8d0",
   "metadata": {},
   "source": [
    "# Procurando correlação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83bd5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(train[['sales', 'onpromotion', 'payday', 'holiday', 'dcoilwtico']].corr(), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad545324",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.boxplot(x='payday', y='sales', data=train)\n",
    "plt.title('Vendas fora dos dias do pagamento e dentro deles')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a91613",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.boxplot(x='holiday', y='sales', data=train)\n",
    "plt.title('Vendas nos fora dos feriados e dentro deles')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3780a668",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Agrupando os dados por data e somando as vendas para cada dia\n",
    "daily_sales = train.groupby('date').agg({\n",
    "    'sales': 'sum',\n",
    "    'dcoilwtico': 'mean'  # Presumindo que o preço do óleo seja o mesmo para todo o dia, ou você pode escolher outra forma de agregá-lo.\n",
    "}).reset_index()\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.scatterplot(x=daily_sales['dcoilwtico'], y=daily_sales['sales'])\n",
    "plt.xlabel('Preço do óleo')\n",
    "plt.ylabel('Vendas Diárias Totais')\n",
    "plt.title('Relação entre o Preço do Óleo e as Vendas Diárias Totais')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f16c6f",
   "metadata": {},
   "source": [
    "Parece que quanto maior o valor do preço do óleo, menor o número de vendas totais. Talvez a alta no preço do óleo cause inflação nos produtos e acabe diminuindo as vendas totais."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dcf996",
   "metadata": {},
   "source": [
    "# Analisando vendas totais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4511ffd2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "a = train[[\"store_nbr\", \"sales\"]]\n",
    "a[\"ind\"] = 1\n",
    "a[\"ind\"] = a.groupby(\"store_nbr\").ind.cumsum().values\n",
    "a = pd.pivot(a, index = \"ind\", columns = \"store_nbr\", values = \"sales\").corr()\n",
    "mask = np.triu(a.corr())\n",
    "plt.figure(figsize=(20, 20))\n",
    "sns.heatmap(a,\n",
    "        annot=True,\n",
    "        fmt='.1f',\n",
    "        cmap='coolwarm',\n",
    "        square=True,\n",
    "        mask=mask,\n",
    "        linewidths=1,\n",
    "        cbar=False)\n",
    "plt.title(\"Correlations among stores\",fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44724e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = train.set_index(\"date\").groupby(\"store_nbr\").resample(\"D\").sales.sum().reset_index()\n",
    "px.line(a, x = \"date\", y= \"sales\", color = \"store_nbr\", title = \"Daily total sales of the stores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efdd890",
   "metadata": {},
   "source": [
    "## Analisei as lojas que demoraram abrir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c753a663",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape)\n",
    "train = train[~((train.store_nbr == 52) & (train.date < \"2017-04-20\"))]\n",
    "train = train[~((train.store_nbr == 22) & (train.date < \"2015-10-09\"))]\n",
    "train = train[~((train.store_nbr == 42) & (train.date < \"2015-08-21\"))]\n",
    "train = train[~((train.store_nbr == 21) & (train.date < \"2015-07-24\"))]\n",
    "train = train[~((train.store_nbr == 29) & (train.date < \"2015-03-20\"))]\n",
    "train = train[~((train.store_nbr == 20) & (train.date < \"2015-02-13\"))]\n",
    "train = train[~((train.store_nbr == 53) & (train.date < \"2014-05-29\"))]\n",
    "train = train[~((train.store_nbr == 36) & (train.date < \"2013-05-09\"))]\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc82e99a",
   "metadata": {},
   "source": [
    "Vou procurar pelas lojas que nunca venderam nada, para usar 0 nos valores de forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c560b771",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c = train.groupby([\"store_nbr\", \"family\"]).sales.sum().reset_index().sort_values([\"family\",\"store_nbr\"])\n",
    "c = c[c.sales == 0]\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0171f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object and col_type.name != 'datetime64[ns]' and col_type.name != 'category':\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "        elif col_type.name == 'category':\n",
    "            df[col] = df[col].cat.as_ordered()\n",
    "            \n",
    "        print(f'Column: {col}\\nDtype before: {col_type}\\nDtype after: {df[col].dtype}')\n",
    "        print(30 * '-')\n",
    "            \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7b7566",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c = reduce_mem_usage(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62307e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "print(train.shape)\n",
    "# Anti Join\n",
    "outer_join = train.merge(c[c.sales == 0].drop(\"sales\",axis = 1), how = 'outer', indicator = True)\n",
    "train = outer_join[~(outer_join._merge == 'both')].drop('_merge', axis = 1)\n",
    "del outer_join\n",
    "gc.collect()\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a9e077",
   "metadata": {},
   "source": [
    "## Preenchendo previsão com 0 das lojas que nunca venderam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8db2e20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "zero_prediction = []\n",
    "for i in range(0,len(c)):\n",
    "    zero_prediction.append(\n",
    "        pd.DataFrame({\n",
    "            \"date\":pd.date_range(\"2017-08-16\", \"2017-08-31\").tolist(),\n",
    "            \"store_nbr\":c.store_nbr.iloc[i],\n",
    "            \"family\":c.family.iloc[i],\n",
    "            \"sales\":0\n",
    "        })\n",
    "    )\n",
    "zero_prediction = pd.concat(zero_prediction)\n",
    "del c\n",
    "gc.collect()\n",
    "zero_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b67430",
   "metadata": {},
   "source": [
    "## Analisando vendas pela família de produtos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bc2e70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = train.groupby(\"family\").sales.mean().sort_values(ascending = False).reset_index()\n",
    "px.bar(a, y = \"family\", x=\"sales\", color = \"family\", title = \"Which product family preferred more?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13773ed",
   "metadata": {},
   "source": [
    "### Analisando se existe correlação com vendas e promoção."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b4a0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Spearman Correlation between Sales and Onpromotion: {:,.4f}\".format(train.corr(\"spearman\").sales.loc[\"onpromotion\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951befa8",
   "metadata": {},
   "source": [
    "Certo, as promoções promovem as vendas, mas precisamos saber se as lojas podem ser muito diferentes entre si."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9460052",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "d = pd.merge(train, stores)\n",
    "d[\"store_nbr\"] = d[\"store_nbr\"].astype(\"int8\")\n",
    "d[\"year\"] = d.date.dt.year\n",
    "px.line(d.groupby([\"city\", \"year\"]).sales.mean().reset_index(), x = \"year\", y = \"sales\", color = \"city\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c79affc",
   "metadata": {},
   "source": [
    "What a mess! Probably, you are confused due to the holidays and events data. It contains a lot of information inside but, don't worry. You just need to take a breathe and think! It is a meta-data so you have to split it logically and make the data useful.\n",
    "\n",
    "What are our problems?\n",
    "\n",
    "Some national holidays have been transferred.\n",
    "There might be a few holidays in one day. When we merged all of data, number of rows might increase. We don't want duplicates.\n",
    "What is the scope of holidays? It can be regional or national or local. You need to split them by the scope.\n",
    "Work day issue\n",
    "Some specific events\n",
    "Creating new features etc.\n",
    "End of the section, they won't be a problem anymore!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1406c929",
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays_events[\"date\"] = pd.to_datetime(holidays_events.date)\n",
    "\n",
    "# holidays[holidays.type == \"Holiday\"]\n",
    "# holidays[(holidays.type == \"Holiday\") & (holidays.transferred == True)]\n",
    "\n",
    "# Transferred Holidays\n",
    "tr1 = holidays_events[(holidays_events.type == \"Holiday\") & (holidays_events.transferred == True)].drop(\"transferred\", axis = 1).reset_index(drop = True)\n",
    "tr2 = holidays_events[(holidays_events.type == \"Transfer\")].drop(\"transferred\", axis = 1).reset_index(drop = True)\n",
    "tr = pd.concat([tr1,tr2], axis = 1)\n",
    "tr = tr.iloc[:, [5,1,2,3,4]]\n",
    "\n",
    "holidays = holidays_events[(holidays_events.transferred == False) & (holidays_events.type != \"Transfer\")].drop(\"transferred\", axis = 1)\n",
    "holidays = holidays.append(tr).reset_index(drop = True)\n",
    "\n",
    "\n",
    "# Additional Holidays\n",
    "holidays[\"description\"] = holidays[\"description\"].str.replace(\"-\", \"\").str.replace(\"+\", \"\").str.replace('\\d+', '')\n",
    "holidays[\"type\"] = np.where(holidays[\"type\"] == \"Additional\", \"Holiday\", holidays[\"type\"])\n",
    "\n",
    "# Bridge Holidays\n",
    "holidays[\"description\"] = holidays[\"description\"].str.replace(\"Puente \", \"\")\n",
    "holidays[\"type\"] = np.where(holidays[\"type\"] == \"Bridge\", \"Holiday\", holidays[\"type\"])\n",
    "\n",
    " \n",
    "# Work Day Holidays, that is meant to payback the Bridge.\n",
    "work_day = holidays[holidays.type == \"Work Day\"]  \n",
    "holidays = holidays[holidays.type != \"Work Day\"]  \n",
    "\n",
    "\n",
    "# Split\n",
    "\n",
    "# Events are national\n",
    "events = holidays[holidays.type == \"Event\"].drop([\"type\", \"locale\", \"locale_name\"], axis = 1).rename({\"description\":\"events\"}, axis = 1)\n",
    "\n",
    "holidays = holidays[holidays.type != \"Event\"].drop(\"type\", axis = 1)\n",
    "regional = holidays[holidays.locale == \"Regional\"].rename({\"locale_name\":\"state\", \"description\":\"holiday_regional\"}, axis = 1).drop(\"locale\", axis = 1).drop_duplicates()\n",
    "national = holidays[holidays.locale == \"National\"].rename({\"description\":\"holiday_national\"}, axis = 1).drop([\"locale\", \"locale_name\"], axis = 1).drop_duplicates()\n",
    "local = holidays[holidays.locale == \"Local\"].rename({\"description\":\"holiday_local\", \"locale_name\":\"city\"}, axis = 1).drop(\"locale\", axis = 1).drop_duplicates()\n",
    "\n",
    "\n",
    "\n",
    "d = pd.merge(train.append(test), stores)\n",
    "d[\"store_nbr\"] = d[\"store_nbr\"].astype(\"int8\")\n",
    "\n",
    "\n",
    "# National Holidays & Events\n",
    "#d = pd.merge(d, events, how = \"left\")\n",
    "d = pd.merge(d, national, how = \"left\")\n",
    "# Regional\n",
    "d = pd.merge(d, regional, how = \"left\", on = [\"date\", \"state\"])\n",
    "# Local\n",
    "d = pd.merge(d, local, how = \"left\", on = [\"date\", \"city\"])\n",
    "\n",
    "# Work Day: It will be removed when real work day colum created\n",
    "d = pd.merge(d,  work_day[[\"date\", \"type\"]].rename({\"type\":\"IsWorkDay\"}, axis = 1),how = \"left\")\n",
    "\n",
    "# EVENTS\n",
    "events[\"events\"] =np.where(events.events.str.contains(\"futbol\"), \"Futbol\", events.events)\n",
    "\n",
    "def one_hot_encoder(df, nan_as_category=True):\n",
    "    original_columns = list(df.columns)\n",
    "    categorical_columns = df.select_dtypes([\"category\", \"object\"]).columns.tolist()\n",
    "    # categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    df = pd.get_dummies(df, columns=categorical_columns, dummy_na=nan_as_category)\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    df.columns = df.columns.str.replace(\" \", \"_\")\n",
    "    return df, df.columns.tolist()\n",
    "\n",
    "events, events_cat = one_hot_encoder(events, nan_as_category=False)\n",
    "events[\"events_Dia_de_la_Madre\"] = np.where(events.date == \"2016-05-08\", 1,events[\"events_Dia_de_la_Madre\"])\n",
    "events = events.drop(239)\n",
    "\n",
    "d = pd.merge(d, events, how = \"left\")\n",
    "d[events_cat] = d[events_cat].fillna(0)\n",
    "\n",
    "# New features\n",
    "d[\"holiday_national_binary\"] = np.where(d.holiday_national.notnull(), 1, 0)\n",
    "d[\"holiday_local_binary\"] = np.where(d.holiday_local.notnull(), 1, 0)\n",
    "d[\"holiday_regional_binary\"] = np.where(d.holiday_regional.notnull(), 1, 0)\n",
    "\n",
    "# \n",
    "d[\"national_independence\"] = np.where(d.holiday_national.isin(['Batalla de Pichincha',  'Independencia de Cuenca', 'Independencia de Guayaquil', 'Independencia de Guayaquil', 'Primer Grito de Independencia']), 1, 0)\n",
    "d[\"local_cantonizacio\"] = np.where(d.holiday_local.str.contains(\"Cantonizacio\"), 1, 0)\n",
    "d[\"local_fundacion\"] = np.where(d.holiday_local.str.contains(\"Fundacion\"), 1, 0)\n",
    "d[\"local_independencia\"] = np.where(d.holiday_local.str.contains(\"Independencia\"), 1, 0)\n",
    "\n",
    "\n",
    "holidays, holidays_cat = one_hot_encoder(d[[\"holiday_national\",\"holiday_regional\",\"holiday_local\"]], nan_as_category=False)\n",
    "d = pd.concat([d.drop([\"holiday_national\",\"holiday_regional\",\"holiday_local\"], axis = 1),holidays], axis = 1)\n",
    "\n",
    "he_cols = d.columns[d.columns.str.startswith(\"events\")].tolist() + d.columns[d.columns.str.startswith(\"holiday\")].tolist() + d.columns[d.columns.str.startswith(\"national\")].tolist()+ d.columns[d.columns.str.startswith(\"local\")].tolist()\n",
    "d[he_cols] = d[he_cols].astype(\"int8\")\n",
    "\n",
    "d[[\"family\", \"city\", \"state\", \"type\"]] = d[[\"family\", \"city\", \"state\", \"type\"]].astype(\"category\")\n",
    "\n",
    "del holidays, holidays_cat, work_day, local, regional, national, events, events_cat, tr, tr1, tr2, he_cols\n",
    "gc.collect()\n",
    "\n",
    "d.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e56a08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Time Related Features\n",
    "def create_date_features(df):\n",
    "    df['month'] = df.date.dt.month.astype(\"int8\")\n",
    "    df['day_of_month'] = df.date.dt.day.astype(\"int8\")\n",
    "    df['day_of_year'] = df.date.dt.dayofyear.astype(\"int16\")\n",
    "    df['week_of_month'] = (df.date.apply(lambda d: (d.day-1) // 7 + 1)).astype(\"int8\")\n",
    "    df['week_of_year'] = (df.date.dt.weekofyear).astype(\"int8\")\n",
    "    df['day_of_week'] = (df.date.dt.dayofweek + 1).astype(\"int8\")\n",
    "    df['year'] = df.date.dt.year.astype(\"int32\")\n",
    "    df[\"is_wknd\"] = (df.date.dt.weekday // 4).astype(\"int8\")\n",
    "    df[\"quarter\"] = df.date.dt.quarter.astype(\"int8\")\n",
    "    df['is_month_start'] = df.date.dt.is_month_start.astype(\"int8\")\n",
    "    df['is_month_end'] = df.date.dt.is_month_end.astype(\"int8\")\n",
    "    df['is_quarter_start'] = df.date.dt.is_quarter_start.astype(\"int8\")\n",
    "    df['is_quarter_end'] = df.date.dt.is_quarter_end.astype(\"int8\")\n",
    "    df['is_year_start'] = df.date.dt.is_year_start.astype(\"int8\")\n",
    "    df['is_year_end'] = df.date.dt.is_year_end.astype(\"int8\")\n",
    "    # 0: Winter - 1: Spring - 2: Summer - 3: Fall\n",
    "    df[\"season\"] = np.where(df.month.isin([12,1,2]), 0, 1)\n",
    "    df[\"season\"] = np.where(df.month.isin([6,7,8]), 2, df[\"season\"])\n",
    "    df[\"season\"] = pd.Series(np.where(df.month.isin([9, 10, 11]), 3, df[\"season\"])).astype(\"int8\")\n",
    "    return df\n",
    "d = create_date_features(d)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Workday column\n",
    "d[\"workday\"] = np.where((d.holiday_national_binary == 1) | (d.holiday_local_binary==1) | (d.holiday_regional_binary==1) | (d['day_of_week'].isin([6,7])), 0, 1)\n",
    "d[\"workday\"] = pd.Series(np.where(d.IsWorkDay.notnull(), 1, d[\"workday\"])).astype(\"int8\")\n",
    "d.drop(\"IsWorkDay\", axis = 1, inplace = True)\n",
    "\n",
    "# Wages in the public sector are paid every two weeks on the 15 th and on the last day of the month. \n",
    "# Supermarket sales could be affected by this.\n",
    "d[\"wageday\"] = pd.Series(np.where((d['is_month_end'] == 1) | (d[\"day_of_month\"] == 15), 1, 0)).astype(\"int8\")\n",
    "\n",
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5b97f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = reduce_mem_usage(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc2372d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "d.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dacda1",
   "metadata": {},
   "source": [
    "# Aplicando o modelo ao XGB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cef7bb",
   "metadata": {},
   "source": [
    "## Descobrindo parâmetros corretos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e90e91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3403c05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(d['dcoilwtico'].dtype)\n",
    "print(d['dcoilwtico'].isna().sum())\n",
    "d['dcoilwtico'] = d['dcoilwtico'].astype('float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687b6687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preencher NaNs na coluna 'dcoilwtico' com o último valor válido\n",
    "d['dcoilwtico']=d['dcoilwtico'].fillna(method='ffill')\n",
    "d['dcoilwtico']=d['dcoilwtico'].fillna(method='bfill')\n",
    "\n",
    "# Excluir a coluna 'dcoilwtico_interpolated'\n",
    "d.drop('dcoilwtico_interpolated', axis=1, inplace=True)\n",
    "\n",
    "# Preencher NaNs na coluna 'earthquake_effect' com 0\n",
    "d['earthquake_effect'].fillna(0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471ec807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the data into train+validation set and forecast set based on 'sales' column\n",
    "train_valid_df = d[d['sales'].notna()]\n",
    "forecast_df = d[d['sales'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdb8d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "split_date = '2017-01-01'  # specify the split date in format 'YYYY-MM-DD'\n",
    "train_df = train_valid_df.loc[train_valid_df['date'] <= split_date]\n",
    "valid_df = train_valid_df.loc[train_valid_df['date'] > split_date]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c69bf5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835e9f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the sum of MAPE scores to zero\n",
    "total_mape_sum = 0\n",
    "\n",
    "# Initialize the counter for combinations of store_nbr and family\n",
    "num_combinations = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3878b33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "exog_cols = [col for col in d.columns if col not in ['id', 'sales', 'date', 'store_nbr', 'family', 'city', 'state', 'type']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bcbd0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "# Initialize an empty DataFrame to store final predictions\n",
    "final_predictions = pd.DataFrame()\n",
    "\n",
    "# Loop through each unique store\n",
    "for store in d['store_nbr'].unique():\n",
    "    \n",
    "    # Find the unique families associated with the current store\n",
    "    unique_families_for_store = d[d['store_nbr'] == store]['family'].unique()\n",
    "    \n",
    "    # Loop through each unique family for the current store\n",
    "    for family in unique_families_for_store:\n",
    "        \n",
    "        # Filter the data for the current store and family\n",
    "        temp_df = d[(d['store_nbr'] == store) & (d['family'] == family)]\n",
    "        \n",
    "        # Separate rows for training (where 'sales' is not NaN) and for prediction (where 'sales' is NaN)\n",
    "        train_data = temp_df.dropna(subset=['sales'])\n",
    "        prediction_data = temp_df[temp_df['sales'].isna()]\n",
    "        \n",
    "        # Check if train_data has data\n",
    "        if len(train_data) == 0:\n",
    "            print(f'No training data for Store {store}, Family {family}. Skipping...')\n",
    "            continue\n",
    "        \n",
    "        # Separate features and target in training data\n",
    "        X = train_data[exog_cols]\n",
    "        y = train_data['sales']\n",
    "        \n",
    "        # Split the training data into train and validation sets\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "        \n",
    "        # Initialize and train the XGBoost model\n",
    "        reg = xgb.XGBRegressor(\n",
    "            objective='reg:squarederror',\n",
    "            n_estimators=50,\n",
    "            max_depth=5,\n",
    "            learning_rate=0.1,\n",
    "            colsample_bytree=0.8\n",
    "        )\n",
    "        \n",
    "        reg.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], early_stopping_rounds=10, verbose=False)\n",
    "        \n",
    "        # Prepare the features for the rows with NaN sales\n",
    "        X_predict = prediction_data[exog_cols]\n",
    "        \n",
    "        # Predict the NaN sales values using the trained model\n",
    "        final_predictions_for_family = reg.predict(X_predict)\n",
    "        \n",
    "        # Add the predictions to the prediction_data DataFrame\n",
    "        prediction_data['sales'] = final_predictions_for_family\n",
    "        \n",
    "        # Append this prediction_data to the final_predictions DataFrame\n",
    "        final_predictions = pd.concat([final_predictions, prediction_data])\n",
    "\n",
    "# At this point, the final_predictions DataFrame contains the rows with the NaN sales that have been filled in.\n",
    "# Merge this back to the original data if needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7183ff2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_predictions.set_index('id').sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6de0fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# which is non-NaN only where we had a match in zero_prediction\n",
    "merged_predictions = pd.merge(\n",
    "    final_predictions.reset_index(), \n",
    "    zero_prediction, \n",
    "    on=['date', 'store_nbr', 'family'], \n",
    "    how='left', \n",
    "    suffixes=('', '_zero')\n",
    ")\n",
    "\n",
    "# Step 2: Update 'sales' in final_predictions to be 0 \n",
    "# wherever a match was found in zero_prediction\n",
    "merged_predictions.loc[merged_predictions['sales_zero'].notna(), 'sales'] = 0\n",
    "\n",
    "# Step 3: Clean up the merged DataFrame to remove the extra columns\n",
    "final_clean_predictions = merged_predictions.drop(columns=['sales_zero'])\n",
    "\n",
    "# Step 4: Fill missing 'sales' values with 0, if any\n",
    "final_clean_predictions['sales'].fillna(0, inplace=True)\n",
    "\n",
    "# Step 5: Restore the original index of final_predictions\n",
    "final_clean_predictions.set_index('id', inplace=True)\n",
    "\n",
    "# Display the updated predictions\n",
    "# Display the updated predictions\n",
    "final_clean_predictions=final_clean_predictions.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaad3fbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_clean_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4a2fe5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Only keep 'id' and 'sales' columns\n",
    "final_clean_predictions = final_clean_predictions[['sales']]\n",
    "# Create a DataFrame with a complete range of index values\n",
    "full_index_range = pd.DataFrame({\n",
    "    'id': np.arange(3000888, 3029400),  # 3029400 is exclusive\n",
    "    'predicted_sales': 0\n",
    "})\n",
    "\n",
    "# Merge the DataFrames\n",
    "final_output = pd.merge(\n",
    "    full_index_range, \n",
    "    final_clean_predictions, \n",
    "    left_on='id', \n",
    "    right_index=True, \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# For indices that were in full_index_range but not in final_clean_predictions, \n",
    "# the 'predicted_sales' column from final_clean_predictions will be NaN. \n",
    "# We fill these NaNs with 0.\n",
    "final_output['predicted_sales'].fillna(0, inplace=True)\n",
    "\n",
    "# Set 'id' as the index\n",
    "final_output.set_index('id', inplace=True)\n",
    "\n",
    "# Display the updated predictions\n",
    "print(final_output)\n",
    "final_output['sales']=final_output['sales'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23d5125",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output=final_output[['sales']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a031677",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output.to_csv('submission_kaggle.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6b26e7",
   "metadata": {},
   "source": [
    "# Observando Fator de inflação da variância:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1691bf96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate VIF for each explanatory variable\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['feature'] = X.columns\n",
    "vif_data['VIF'] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))]\n",
    "\n",
    "# Inspect the values\n",
    "vif_data.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d09bc4a",
   "metadata": {},
   "source": [
    "# Usando Bayes Optimization to figure out what is the best paramters for each family:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b889abac",
   "metadata": {},
   "source": [
    "# Eficácia piorou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d6d952",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "\n",
    "def xgb_eval(learning_rate, max_depth, n_estimators, colsample_bytree, subsample):\n",
    "    model = xgb.XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        learning_rate=max(learning_rate, 0),\n",
    "        max_depth=int(max_depth),\n",
    "        n_estimators=int(n_estimators),\n",
    "        colsample_bytree=max(min(colsample_bytree, 1), 0),\n",
    "        subsample=max(min(subsample, 1), 0),\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    model.fit(\n",
    "        X_train, \n",
    "        y_train, \n",
    "        eval_set=[(X_valid, y_valid)], \n",
    "        eval_metric='rmse', \n",
    "        early_stopping_rounds=10, \n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Here is how you access the evaluation results\n",
    "    evals_result = model.evals_result()\n",
    "    best_score = evals_result['validation_0']['rmse'][-1]\n",
    "    \n",
    "    return -best_score\n",
    "\n",
    "def tune_and_predict(store, family, data, exog_cols):\n",
    "    temp_df = data[(data['store_nbr'] == store) & (data['family'] == family)]\n",
    "    train_data = temp_df.dropna(subset=['sales'])\n",
    "    prediction_data = temp_df[temp_df['sales'].isna()]\n",
    "    \n",
    "    if len(train_data) == 0:\n",
    "        print(f'No training data for Store {store}, Family {family}. Skipping...')\n",
    "        return None\n",
    "\n",
    "    global X_train, X_valid, y_train, y_valid\n",
    "    X = train_data[exog_cols]\n",
    "    y = train_data['sales']\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    bounds = {\n",
    "        'learning_rate': (0.01, 0.3),\n",
    "        'max_depth': (3, 8),\n",
    "        'n_estimators': (10, 100),\n",
    "        'colsample_bytree': (0.7, 1),\n",
    "        'subsample': (0.7, 1)\n",
    "    }\n",
    "\n",
    "    optimizer = BayesianOptimization(f=xgb_eval, pbounds=bounds, random_state=42)\n",
    "    optimizer.maximize(init_points=3, n_iter=5)\n",
    "    \n",
    "    best_params = optimizer.max['params']\n",
    "    best_params['max_depth'] = int(best_params['max_depth'])\n",
    "    best_params['n_estimators'] = int(best_params['n_estimators'])\n",
    "\n",
    "    model = xgb.XGBRegressor(objective='reg:squarederror', **best_params, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    X_predict = prediction_data[exog_cols]\n",
    "    prediction_data['sales'] = model.predict(X_predict)\n",
    "    \n",
    "    return prediction_data\n",
    "\n",
    "# Example usage:\n",
    "final_predictions = pd.DataFrame()\n",
    "\n",
    "for store in d['store_nbr'].unique():\n",
    "    unique_families_for_store = d[d['store_nbr'] == store]['family'].unique()\n",
    "    for family in unique_families_for_store:\n",
    "        predictions = tune_and_predict(store, family, d, exog_cols)\n",
    "        if predictions is not None:\n",
    "            final_predictions = pd.concat([final_predictions, predictions])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c18fa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06be53ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
